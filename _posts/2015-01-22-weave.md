---
layout: post
title: Adventures with Weave and Docker
---

In the Docker world one always feels like being at the edge of the development. After orchestration slowly gets settled with tools like Mesos, fleet or Kubernetes, the next big topic is networking.

## Networking â€“ the next big thing in the Docker world

Docker started out with a simple one-host networking solution. Unfortunately, this does not really scale to multiple hosts. A number of projects attack this problem, to name a few: [socketplane](https://github.com/socketplane), [weave](https://github.com/zettio/weave), pipeworks, flannel.

What they have in common is that they try to get rid of the port management which one usually has to do when running multiple apps in a Docker environment: if you have two webservers, you cannot reuse port 80 on the same host. Hence, in many former setups (like with Mesos Marathon) one had to choose (or let the orchestrator choose) different host port which were mapped to port 80 of the webserver inside of Docker containers.

Now, with those new networking solutions each container gets its own IP address. Consequently, every webserver inside of a container can again listen to port 80 and other services can access those webservers via port 80 and the correct IP.

## Towards container IPs with Mesos Marathon

In the following I will describe the way to use Weave to assign each Mesos Marathon container it's own IP address. All Mesos **tasks will live in a Weave overlay network 10.0.0.0/8** which is shared among all hosts running the Mesos slaves.

With Weave it's possible to define different networks for different apps. For the sake of simplicity **we will not implement any network separation here**. Instead we choose one overlay network for all tasks. By doing this we get IP management for free from the Docker deamon. You will read in a minute how.

With Weave one usually runs containers using the `weave run` wrapper script which runs `docker run`and then attaches the `weave` bridge to the container. We will go another route and **use the `weave` bridge directly as Docker bridge**, i.e. as replacement of the `docker0` bridge. This way we will get rid of the problems around this wrapper hack (i.e. that the container process will start before the `weave` bridge is attached). In the future, with the integration of network plugins into Docker, this wrapper will go away. But in the current state using the wrapper is not an option.

## 0. Setting up weave

Using Weave is pretty simple:

- download the weave shell script:

```shell
$ sudo wget -O /usr/local/bin/weave \
  https://github.com/zettio/weave/releases/download/latest_release/weave
$ sudo chmod a+x /usr/local/bin/weave
```

- starting the router container:

```shell
$ weave launch
```

- put the host into the 10.0.0.0/8 network:

```shell
$ weave expose 10.0.0.1/8
$ ifconfig weave
weave     Link encap:Ethernet  HWaddr 7a:d2:8d:b1:26:7b  
          inet addr:10.0.0.1  Bcast:0.0.0.0  Mask:255.0.0.0
          UP BROADCAST MULTICAST  MTU:65535  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)
$ brctl show
bridge name     bridge id               STP enabled     interfaces
weave           8000.7ad28db1267b       no
```

- start a container:

```shell
$ weave run 10.0.0.2/8 -itd sttts/python-ubuntu python -m SimpleHTTPServer 80
$ curl 10.0.0.2
```

## 1. Patching the weave shell script

We want to use the `weave` bridge directly as Docker bridge (using the `-b weave` parameter for the Docker daemon). The `weave` script creates the `weave` bridge on `launch`. But in order to run `weave launch` Docker must be running.

It is easy to patch the `weave` script such that the `weave` bridge can created without a running Docker. A pull request exists to add this to `weave`: [https://github.com/zettio/weave/pull/357](https://github.com/zettio/weave/pull/357). Until this is merged a patched version is available here: [https://raw.githubusercontent.com/sttts/weave/888edd69f3344006dac7865a0afe567ebfaa5967/weave](https://raw.githubusercontent.com/sttts/weave/888edd69f3344006dac7865a0afe567ebfaa5967/weave)

With this in place in `/usr/local/bin/weave` we create the `weave` bridge:

```shell
weave setup-bridge
```

## 2. Configuring Docker

Then we change the Docker daemon parameters accordingly, on Ubuntu this is done in `/etc/default/docker`:

```shell
DOCKER_OPTS="--bridge=weave --fixed-cidr=10.1.0.0/16"
```

We have also set the `fixed-cidr` value which defines where the containers of this host will live. The given network is a subnet of 10.0.0.0/8 which is must be exclusive for this host. In a bigger Mesos cluster you have to configure disjunct subnets of course.

Now start the Docker daemon:

```
$ start docker
```

New containers will get a new IP address is the `fixed-cidr` address range. Notice that we offload the IP management for every host to Docker. Because the subnet above are selected properly we get cluster wide unique IP addresses. Fantastic!

```shell
$ docker run -it ubuntu /bin/bash
$ ping 10.0.0.1
```

## 3. Automatic startup of the weave bridge

Everything done above can easily be scriped using Debian/Ubuntu's interfaces definitions:

```
auto weave
iface weave inet manual
        pre-up /usr/local/bin/weave setup-bridge
        post-up /usr/local/bin/weave expose 10.0.0.1/8
        pre-down /usr/local/bin/weave hide 10.0.0.1/8 && ifconfig weave down
        post-down brctl delbr weave
```

We can use `ifup` and `ifdown` to start and stop the script:

```
ifup weave
ifdown weave
```

As the interface is defined as `auto` it will come up by itself after a reboot. Because `/etc/init/docker.conf` defines a dependency on the network interfaces, the Docker daemon will properly wait for the `weave` bridge as well.

## 4. Launching the router

To connect multiple hosts in one giant `10.0.0.0/8` network we have to start the weave router container:

```
$ weave launch
```

Then you can connect hosts via e.g.

```
$ weave connect 192.168.0.42
```

Note the followin:

- **All containers** and hosts **can "see" eachother** via the weave network.
- **All ports** that container processes listen to on the weave network interface **will be accessible by all containers**.
- Those ports are **not** expose outside of the weave network. This can be done by exposing them with the means of Docker e.g.
```
docker run -itd -p 12345:80 sttts/python-ubuntu python -m SimpleHTTPServer 80
```
The webserver will be accessible via the weave network address, e.g. `10.1.0.1:80` and via `192.168.0.41:12345` if the later is the host interface IP. Via `10.1.0.1:80` it can even be accessed on every node in the cluster.

## 5. Service discovery with Consul and progrium/registrator

In other setups I have used the combination of [Consul](http://consul.io) and Jeff Lindsay's great [registrator tool](https://github.com/progrium/registrator) to get service discovery via normal DNS. 

They are pretty simple to get up an running. You start consul as usual, e.g. in a one-host test setup like
```shell
$ consul agent -server -node=goedel -bootstrap -data-dir ./data -client=0.0.0.0
```
and then you can point the registrator container Consul's address, e.g. `192.168.0.41` with
```shell
$ docker run -it -v /var/run/docker.sock:/tmp/docker.sock sttts/registrator consul://192.168.0.41:8500
```

Now, Consul's DNS server can be queried to find out about the running containers, e.g. the `SimpleHTTPServer` container from above:
```shell
$ dig @192.168.0.41 -p 8600 python-ubuntu.service.dc1.consul. ANY
; <<>> DiG 9.8.3-P1 <<>> @192.168.0.41 -p 8600 python-ubuntu.service.dc1.consul. ANY
; (1 server found)
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 45546
;; flags: qr aa rd; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 0
;; WARNING: recursion requested but not available

;; QUESTION SECTION:
;python-ubuntu.service.dc1.consul. IN   ANY

;; ANSWER SECTION:
python-ubuntu.service.dc1.consul. 0 IN  A       192.168.0.41

;; Query time: 3 msec
;; SERVER: 192.168.0.41#8600(127.0.0.1)
;; WHEN: Thu Jan 22 17:30:09 2015
;; MSG SIZE  rcvd: 146
```

As you see the problem is that Consul will return the public node ID, more precisely the Consul agent advertised IP. This is not what we want.

Luckily, in Consul's git master branch 2 weeks ago a new feature of custom service addresses was merged: [https://github.com/hashicorp/consul/pull/570](https://github.com/hashicorp/consul/pull/570). 

But the very first sentence of this post strikes again. The features was merged, but the golang API in the same repository (used by registrator) does not support the Address extension of the service definition yet.

But, as Consul is open source it is easy to make a pull request for this: [https://github.com/hashicorp/consul/pull/627](https://github.com/hashicorp/consul/pull/627). 

With this patch we can extend registrator with a few lines of code to support the `-internal` flag also for Consul (it had turned out that registrator already knows about internal container addresses but due to the lack of support in Consul, it was only implemented for etcd). So, here is another pull request, this time against registrator: [https://github.com/progrium/registrator/pull/91](https://github.com/progrium/registrator/pull/91).

With this in place we can use the extended registrator and get proper, cluster-wide routable weave network addresses:
```
$ docker run -it -v /var/run/docker.sock:/tmp/docker.sock sttts/registrator -internal consul://192.168.0.41:8500
```
and then the DNS response is much better:
```shell
```shell
$ dig @192.168.0.41 -p 8600 python-ubuntu.service.dc1.consul. ANY
; <<>> DiG 9.8.3-P1 <<>> @192.168.0.41 -p 8600 python-ubuntu.service.dc1.consul. ANY
; (1 server found)
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 41271
;; flags: qr aa rd; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 0
;; WARNING: recursion requested but not available

;; QUESTION SECTION:
;python-ubuntu.service.dc1.consul. IN   ANY

;; ANSWER SECTION:
python-ubuntu.service.dc1.consul. 0 IN  A       10.1.0.1

;; Query time: 0 msec
;; SERVER: 192.168.0.41#8600(127.0.0.1)
;; WHEN: Thu Jan 22 17:32:40 2015
;; MSG SIZE  rcvd: 146
```

## 6. Mesos

TBD